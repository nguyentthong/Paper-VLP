# Papers of Vision-and-Language Pretraining (VLP)

1. **Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning**

1. **Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing**

1. **Fusion of detected objects in text for visual question answering**

1. **Video captioning using deep learning: An overview of methods, datasets and metrics**

1. **Bottom-up and top-down attention for image captioning and visual question answering**

1. **A survey on automatic image caption generation**

1. **Automatic description generation from images: A survey of models, datasets, and evaluation measures**

1. **Data efficient masked language modeling for vision and language**

1. **On the opportunities and risks of foundation models**

1. **Behind the scene: Revealing the secrets of pretrained vision-and-language models**

1. **End-to-end object detection with transformers**

1. **Vlp: A survey on vision-language pre-training**

1. **Attacking visual language grounding with adversarial examples: A case study on neural image captioning**

1. **Uniter: Universal image-text representation learning**

1. **X-lxmert: Paint, caption and answer questions with multi-modal transformers**

1. **Unifying vision-and-language tasks via text generation**

1. **Virtex: Learning visual representations from textual annotations**

1. **An image is worth 16x16 words: Transformers for image recognition at scale**

1. **A survey of vision-language pre-trained models**

1. **Compressing visual-linguistic model via knowledge distillation**

1. **Large-scale adversarial training for visionand-language representation learning**

1. **Playing lottery tickets with vision and language**

1. **Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval**

1. **Vqa-lol: Visual question answering under the lens of logic**

1. **Explaining and harnessing adversarial examples**

1. **Lamp: label augmented multimodal pretraining**

1. **Deep multimodal representation learning: A survey**

1. **Dialog-based interactive image retrieval**

1. **Towards learning a generic agent for vision-andlanguage navigation via pre-training**

1. **Decoupling the role of data, attention, and losses in multimodal transformers**

1. **Vln bert: A recurrent vision-and-language bert for navigation**

1. **Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing**

1. **Iterative answer prediction with pointeraugmented multimodal transformers for textvqa**

1. **A comprehensive survey of deep learning for image captioning**

1. **Vivo: Visual vocabulary pre-training for novel object captioning**

1. **Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models**

1. **Pixel-bert: Aligning image pixels with text by deep multi-modal transformers**

1. **Seeing out of the box: End-to-end pre-training for vision-language representation learning**

1. **Seeing out of the box: End-to-end pre-training for vision-language representation learning**

1. **Mural: multimodal, multitask retrieval across languages**

1. **Scaling up visual and vision-language representation learning with noisy text supervision**

1. **Visual question answering: Datasets, algorithms, and future challenges**

1. **Challenges and prospects in vision and language research**

1. **Roses are red, violets are blue... but should vqa expect them to?**

1. **Video question-answering techniques, benchmark datasets and evaluation metrics leveraging video captioning: A comprehensive survey**

1. **Bilinear attention networks**

1. **Vilt: Vision-and-language transformer without convolution or region supervision**

1. **The open images dataset v4**

1. **Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training**

1. **Align before fuse: Vision and language representation learning with momentum distillation**

1. **Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation**

1. **Relation-aware graph attention network for visual question answering**

1. **A closer look at the robustness of vision-and-language pre-trained models**

1. **Adversarial vqa: A new benchmark for evaluating the robustness of vqa models**

1. **Visualbert: A simple and performant baseline for vision and language**

1. **What does bert with vision look at?**

1. **Unsupervised vision-and-language pre-training without parallel images and captions**

1. **Visual to text: Survey of image and video captioning**

1. **Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning**

1. **Oscar: Object-semantics aligned pre-training for vision-language tasks**

1. **Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond**

1. **Interbert: Vision-and-language interaction for multi-modal pretraining**

1. **Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks**

1. **12-in-1: Multi-task vision and language representation learning**

1. **Large-scale pretraining for visual dialog: A simple state-of-the-art baseline**

1. **M3p: Learning universal representations via multitask multilingual multimodal pre-training**

1. **Advancing state-of-the-art image recognition with deep learning on hashtags**

1. **Bridging vision and language from the video-to-text perspective: A comprehensive review**

1. **Learning transferable visual models from natural language supervision**

1. **Zero-shot text-to-image generation**

1. **Learning visual representations with caption annotations**

1. **Interpretable convolutional neural networks with dual local and global attention for review rating prediction**

1. **Is attention interpretable?**

1. **Reasoning over vision and language: Exploring the benefits of supplemental knowledge**

1. **Contrastive visual-linguistic pretraining**

1. **Textcaps: a dataset for image captioning with reading comprehension**

1. **Are we pretraining it right? Digging deeper into visio-linguistic pretraining**

1. **Flava: A foundational language and vision alignment model**

1. **Worst of both worlds: Biases compound in pre-trained vision-and-language models**

1. **From show to tell: A survey on image captioning**

1. **Vl-bert: Pre-training of generic visual-linguistic representations**

1. **Analyzing compositionality in visual question answering**

1. **Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval**

1. **Lxmert: Learning cross-modality encoder representations from transformers**

1. **Vokenization: Improving language understanding with contextualized, visual-grounded supervision**

1. **Multimodal review generation for recommender systems**

1. **Multimodal few-shot learning with frozen language models**

1. **Minivlm: A smaller and faster vision-language model**

1. **Vlmo: Unified vision-language pre-training with mixture-of-modality-experts**

1. **Vd-bert: A unified vision and dialog transformer with bert**

1. **Simvlm: Simple visual language model pretraining with weak supervision**

1. **Visual question answering: A survey of methods and datasets**

1. **Xgpt: Cross-modal generative pre-training for image captioning**

1. **E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning**

1. **Show, attend and tell: Neural image caption generation with visual attention**

1. **Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training**

1. **Causal attention for vision-language tasks**

1. **Tap: Text-aware pre-training for text-vqa and text-caption**

1. **Ernie-vil: Knowledge enhanced vision-language representations through scene graph**

1. **Multimodal intelligence: Representation learning, information fusion, and applications**

1. **Curriculum learning for vision-and-language navigation**

1. **Vinvl: Revisiting visual representations in vision-language models**

1. **Egnet: Edge guidance network for salient object detection**

1. **Unified vision-language pre-training for image captioning and vqa**

1. **Uc2: Universal cross-lingual cross-modal vision-and-language pre-training**

1. **Kaleido-bert: Vision-language pre-training on fashion domain**