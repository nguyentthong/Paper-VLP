# Papers of Vision-and-Language Pretraining (VLP)

1. **Spatio-temporal dynamics and semantic attribute enriched visual encoding for video captioning** *Nayyer Aafaq, Naveed Akhtar, Wei Liu, Syed Zulqarnain Gilani, Ajmal Mian* `CVPR 2019` [[pdf]](https://arxiv.org/abs/1902.10322)

1. **Towards causal vqa: Revealing and reducing spurious correlations by invariant and covariant semantic editing** *Vedika Agarwal, Rakshith Shetty, Mario Fritz* `arXiv 2019`[[pdf]](https://arxiv.org/abs/1912.07538)

1. **Fusion of detected objects in text for visual question answering** *Chris Alberti, Jeffrey Ling, Michael Collins, David Reitter* `arXiv 2019` [[pdf]](https://arxiv.org/abs/1908.05054) [[code]](https://github.com/google-research/language)

1. **Bottom-up and top-down attention for image captioning and visual question answering** *Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang* `CVPR 2018` [[pdf]](https://arxiv.org/abs/1707.07998) [[code]](https://github.com/peteanderson80/bottom-up-attention)

1. **A survey on automatic image caption generation** *Shuang Bai, Shan An* `Neurocomputing 2018` [[pdf]](https://www.sciencedirect.com/science/article/abs/pii/S0925231218306659) 

1. **Automatic description generation from images: A survey of models, datasets, and evaluation measures** *Raffaella Bernardi, Ruket Cakici, Desmond Elliott, Aykut Erdem, Erkut Erdem, Nazli Ikizler-Cinbis, Frank Keller, Adrian Muscat, Barbara Plank* `Journal of Artificial Intelligence Review 2016`[[pdf]](https://arxiv.org/abs/1601.03896)

1. **Data efficient masked language modeling for vision and language** *Yonatan Bitton, Gabriel Stanovsky, Michael Elhadad, Roy Schwartz* `EMNLP 2021 findings` [[pdf]](https://arxiv.org/abs/2109.02040) [[code]](https://github.com/yonatanbitton/data_efficient_masked_language_modeling_for_vision_and_language)

1. **Behind the scene: Revealing the secrets of pretrained vision-and-language models** *Jize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-Chun Chen, Jingjing Liu* `ECCV 2020` [[pdf]](https://arxiv.org/abs/2005.07310)

1. **End-to-end object detection with transformers** *Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko* `arXiv 2020` [[pdf]](https://arxiv.org/abs/2005.12872) [[code]](https://github.com/facebookresearch/detr)

1. **Vlp: A survey on vision-language pre-training** *Feilong Chen, Duzhen Zhang, Minglun Han, Xiuyi Chen, Jing Shi, Shuang Xu, Bo Xu* `arXiv 2022` [[pdf]](https://arxiv.org/abs/2202.09061) [[code]](https://github.com/phellonchen/awesome-Vision-and-Language-Pre-training)

1. **Attacking visual language grounding with adversarial examples: A case study on neural image captioning** *Hongge Chen, Huan Zhang, Pin-Yu Chen, Jinfeng Yi, Cho-Jui Hsieh* `ACL 2018` [[pdf]](https://arxiv.org/abs/1712.02051) [[code]](https://github.com/huanzhang12/ImageCaptioningAttack)

1. **Uniter: Universal image-text representation learning** *Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu* `ECCV 2020` [[pdf]](https://arxiv.org/abs/1909.11740) [[code]](https://github.com/ChenRocks/UNITER)

1. **X-lxmert: Paint, caption and answer questions with multi-modal transformers** *Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, Aniruddha Kembhavi* `EMNLP 2020` [[pdf]](https://arxiv.org/abs/2009.11278) [[code]](https://github.com/allenai/x-lxmert)

1. **Unifying vision-and-language tasks via text generation** *Jaemin Cho, Jie Lei, Hao Tan, Mohit Bansal* `ICML 2021` [[pdf]](https://arxiv.org/abs/2102.02779) [[code]](https://github.com/j-min/VL-T5)

1. **Virtex: Learning visual representations from textual annotations** *Karan Desai, Justin Johnson* `CVPR 2021` [[pdf]](https://arxiv.org/abs/2006.06666) [[code]](https://github.com/kdexd/virtex)

1. **An image is worth 16x16 words: Transformers for image recognition at scale** *Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby* `ICLR 2020` [[pdf]](https://arxiv.org/abs/2010.11929) [[code]](https://github.com/google-research/vision_transformer)

1. **A survey of vision-language pre-trained models** *Yifan Du, Zikang Liu, Junyi Li, Wayne Xin Zhao* `IJCAI 2022` [[pdf]](https://arxiv.org/abs/2202.10936)

1. **Compressing visual-linguistic model via knowledge distillation** *Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang, Zicheng Liu* `arXiv 2021` [[pdf]](https://arxiv.org/abs/2104.02096)

1. **Large-scale adversarial training for visionand-language representation learning** *Zhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, Jingjing Liu* `NeurIPS 2020` [[pdf]](https://arxiv.org/abs/2006.06195) [[code_1]](https://github.com/zhegan27/VILLA) [[code_2]](https://github.com/zhegan27/LXMERT-AdvTrain)

1. **Playing lottery tickets with vision and language** *Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu Cheng, Shuohang Wang, Jingjing Liu, Lijuan Wang, Zicheng Liu* `AAAI 2022` [[pdf]](https://arxiv.org/abs/2104.11832)

1. **Fashionbert: Text and image matching with adaptive loss for cross-modal retrieval** *Dehong Gao, Linbo Jin, Ben Chen, Minghui Qiu, Peng Li, Yi Wei, Yi Hu, Hao Wang* `SIGIR 2020` [[pdf]](https://arxiv.org/abs/2005.09801)

1. **Vqa-lol: Visual question answering under the lens of logic** *Tejas Gokhale, Pratyay Banerjee, Chitta Baral, Yezhou Yang* `ECCV 2020` [[pdf]](https://arxiv.org/abs/2002.08325)

1. **Lamp: label augmented multimodal pretraining** *Jia Guo, Chen Zhu, Yilun Zhao, Heda Wang, Yao Hu, Xiaofei He, Deng Cai* `` [[pdf]](https://arxiv.org/abs/2012.04446)

1. **Deep multimodal representation learning: A survey** *Wenzhong Guo, Jianwen Wang, Shiping Wang* `IEEE Access 2019` [[pdf]](https://ieeexplore.ieee.org/document/8715409)

1. **Dialog-based interactive image retrieval** *Xiaoxiao Guo, Hui Wu, Yu Cheng, Steven Rennie, Gerald Tesauro, Rogerio Schmidt Feris* `NeurIPS 2018` [[pdf]](https://arxiv.org/abs/1805.00145) [[code]](https://github.com/XiaoxiaoGuo/fashion-retrieval)

1. **Towards learning a generic agent for vision-andlanguage navigation via pre-training** ** `` [[pdf]]() [[code]]()

1. **Decoupling the role of data, attention, and losses in multimodal transformers** ** `` [[pdf]]() [[code]]()

1. **Vln bert: A recurrent vision-and-language bert for navigation** ** `` [[pdf]]() [[code]]()

1. **Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing** ** `` [[pdf]]() [[code]]()

1. **Iterative answer prediction with pointeraugmented multimodal transformers for textvqa** ** `` [[pdf]]() [[code]]()

1. **A comprehensive survey of deep learning for image captioning** ** `` [[pdf]]() [[code]]()

1. **Vivo: Visual vocabulary pre-training for novel object captioning** ** `` [[pdf]]() [[code]]()

1. **Multilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models** ** `` [[pdf]]() [[code]]()

1. **Pixel-bert: Aligning image pixels with text by deep multi-modal transformers** ** `` [[pdf]]() [[code]]()

1. **Seeing out of the box: End-to-end pre-training for vision-language representation learning** ** `` [[pdf]]() [[code]]()

1. **Mural: multimodal, multitask retrieval across languages** ** `` [[pdf]]() [[code]]()

1. **Scaling up visual and vision-language representation learning with noisy text supervision** ** `` [[pdf]]() [[code]]()

1. **Visual question answering: Datasets, algorithms, and future challenges** ** `` [[pdf]]() [[code]]()

1. **Challenges and prospects in vision and language research** ** `` [[pdf]]() [[code]]()

1. **Roses are red, violets are blue... but should vqa expect them to?** ** `` [[pdf]]() [[code]]()

1. **Video question-answering techniques, benchmark datasets and evaluation metrics leveraging video captioning: A comprehensive survey** ** `` [[pdf]]() [[code]]()

1. **Bilinear attention networks** ** `` [[pdf]]() [[code]]()

1. **Vilt: Vision-and-language transformer without convolution or region supervision** ** `` [[pdf]]() [[code]]()

1. **The open images dataset v4** ** `` [[pdf]]() [[code]]()

1. **Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training** ** `` [[pdf]]() [[code]]()

1. **Align before fuse: Vision and language representation learning with momentum distillation** ** `` [[pdf]]() [[code]]()

1. **Blip: Bootstrapping language-image pretraining for unified vision-language understanding and generation** ** `` [[pdf]]() [[code]]()

1. **Relation-aware graph attention network for visual question answering** ** `` [[pdf]]() [[code]]()

1. **A closer look at the robustness of vision-and-language pre-trained models** ** `` [[pdf]]() [[code]]()

1. **Adversarial vqa: A new benchmark for evaluating the robustness of vqa models** ** `` [[pdf]]() [[code]]()

1. **Visualbert: A simple and performant baseline for vision and language** ** `` [[pdf]]() [[code]]()

1. **What does bert with vision look at?** ** `` [[pdf]]() [[code]]()

1. **Unsupervised vision-and-language pre-training without parallel images and captions** [[pdf]]() [[code]]()

1. **Visual to text: Survey of image and video captioning** ** `` [[pdf]]() [[code]]()

1. **Unimo: Towards unified-modal understanding and generation via cross-modal contrastive learning** ** `` [[pdf]]() [[code]]()

1. **Oscar: Object-semantics aligned pre-training for vision-language tasks** ** `` [[pdf]]() [[code]]()

1. **Interpretable deep learning: Interpretation, interpretability, trustworthiness, and beyond** ** `` [[pdf]]() [[code]]()

1. **Interbert: Vision-and-language interaction for multi-modal pretraining** ** `` [[pdf]]() [[code]]()

1. **Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks** ** `` [[pdf]]() [[code]]()

1. **12-in-1: Multi-task vision and language representation learning** ** `` [[pdf]]() [[code]]()

1. **Large-scale pretraining for visual dialog: A simple state-of-the-art baseline** ** `` [[pdf]]() [[code]]()

1. **M3p: Learning universal representations via multitask multilingual multimodal pre-training** ** `` [[pdf]]() [[code]]()

1. **Advancing state-of-the-art image recognition with deep learning on hashtags** ** `` [[pdf]]() [[code]]()

1. **Bridging vision and language from the video-to-text perspective: A comprehensive review** ** `` [[pdf]]() [[code]]()

1. **Learning transferable visual models from natural language supervision** ** `` [[pdf]]() [[code]]()

1. **Zero-shot text-to-image generation** ** `` [[pdf]]() [[code]]()

1. **Learning visual representations with caption annotations** ** `` [[pdf]]() [[code]]()

1. **Interpretable convolutional neural networks with dual local and global attention for review rating prediction** ** `` [[pdf]]() [[code]]()

1. **Is attention interpretable?** ** `` [[pdf]]() [[code]]()

1. **Reasoning over vision and language: Exploring the benefits of supplemental knowledge** ** `` [[pdf]]() [[code]]()

1. **Contrastive visual-linguistic pretraining** ** `` [[pdf]]() [[code]]()

1. **Textcaps: a dataset for image captioning with reading comprehension** ** `` [[pdf]]() [[code]]()

1. **Are we pretraining it right? Digging deeper into visio-linguistic pretraining** ** `` [[pdf]]() [[code]]()

1. **Flava: A foundational language and vision alignment model** ** `` [[pdf]]() [[code]]()

1. **Worst of both worlds: Biases compound in pre-trained vision-and-language models** ** `` [[pdf]]() [[code]]()

1. **From show to tell: A survey on image captioning** ** `` [[pdf]]() [[code]]()

1. **Vl-bert: Pre-training of generic visual-linguistic representations** ** `` [[pdf]]() [[code]]()

1. **Analyzing compositionality in visual question answering** ** `` [[pdf]]() [[code]]()

1. **Lightningdot: Pre-training visual-semantic embeddings for real-time image-text retrieval** ** `` [[pdf]]() [[code]]()

1. **Lxmert: Learning cross-modality encoder representations from transformers** ** `` [[pdf]]() [[code]]()

1. **Vokenization: Improving language understanding with contextualized, visual-grounded supervision** ** `` [[pdf]]() [[code]]()

1. **Multimodal review generation for recommender systems** ** `` [[pdf]]() [[code]]()

1. **Multimodal few-shot learning with frozen language models** ** `` [[pdf]]() [[code]]()

1. **Minivlm: A smaller and faster vision-language model** ** `` [[pdf]]() [[code]]()

1. **Vlmo: Unified vision-language pre-training with mixture-of-modality-experts** ** `` [[pdf]]() [[code]]()

1. **Vd-bert: A unified vision and dialog transformer with bert** ** `` [[pdf]]() [[code]]()

1. **Simvlm: Simple visual language model pretraining with weak supervision** ** `` [[pdf]]() [[code]]()

1. **Visual question answering: A survey of methods and datasets** ** `` [[pdf]]() [[code]]()

1. **Xgpt: Cross-modal generative pre-training for image captioning** ** `` [[pdf]]() [[code]]()

1. **E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning** ** `` [[pdf]]() [[code]]()

1. **Show, attend and tell: Neural image caption generation with visual attention** ** `` [[pdf]]() [[code]]()

1. **Probing inter-modality: Visual parsing with self-attention for vision-and-language pre-training** ** `` [[pdf]]() [[code]]()

1. **Causal attention for vision-language tasks** ** `` [[pdf]]() [[code]]()

1. **Tap: Text-aware pre-training for text-vqa and text-caption** ** `` [[pdf]]() [[code]]()

1. **Ernie-vil: Knowledge enhanced vision-language representations through scene graph** ** `` [[pdf]]() [[code]]()

1. **Multimodal intelligence: Representation learning, information fusion, and applications** ** `` [[pdf]]() [[code]]()

1. **Curriculum learning for vision-and-language navigation** ** `` [[pdf]]() [[code]]()

1. **Vinvl: Revisiting visual representations in vision-language models** ** `` [[pdf]]() [[code]]()

1. **Egnet: Edge guidance network for salient object detection** ** `` [[pdf]]() [[code]]()

1. **Unified vision-language pre-training for image captioning and vqa** ** `` [[pdf]]() [[code]]()

1. **Uc2: Universal cross-lingual cross-modal vision-and-language pre-training** ** `` [[pdf]]() [[code]]()

1. **Kaleido-bert: Vision-language pre-training on fashion domain** ** `` [[pdf]]() [[code]]()